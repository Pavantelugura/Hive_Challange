1) What is the definition of Hive? What is the present version of Hive?
Ans.-->Hive is an open-source data warehouse system for querying and analysing large datasets stored in Hadoop files.
       The present version of Hive- 3.1.3
2) Is Hive suitable to be used for OLTP systems? Why?
Ans.-->No, it does not provide insert and update at row level and it is mainly for write once read many times. So it is not used for OLTP system.

3) How is Hive different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
Ans.-->Hive is data warehouse system and RDBMS is used to maintain databases.
       Hive supports ACID. But in older versions of Hive it does not support ACID transactions on tables, in new versions it supports by default but we have enable it.

4) Explain the Hive Architecture and the different components of a Hive Architecture?
Ans.-->Hive Client supports writing applications in various languages and it supports different types of clients such as hive thrift client, hive JDBC driver, hive ODBC driver.
       Hive Web User Interface- The user interface providing by hive are Hive web ui or hive HD insight. We can submit hive queries directly to hive server with help of these web ui.
       Command Line Interface - This is the default shell provided by hive to execute the hive queries or commands directly.
       Hive Server - This server is based on the thrift language, therefore all the application which support thrift can interact with hive server with different drivers. It allows different clients to submit requests to Hive and retrieve the final result.
       Hive Driver - The driver is used to receive the quires from UI. It provides execute and fetch APIs modeled on JDBC/ODBC interfaces.
       Compiler - This receives query from the driver and perform the parsing, type checking, and semantic analysis with the help of schema present in the metastore.
       Executor - The component which executes the execution plan created by the compiler. The plan is a DAG there is either map or reduce task. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.
       Metastore - This contains metadata for hive tables like schema and location, and partitions in a relational database. Client can access this information by metastore service API.
       Storage Layer - This contains HDFS or HBASE for the storage of data in the tables.
      
5) Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
Ans.-->Hive Query Processor is used to convert SQL to MapReduce jobs, based on order of dependencies, the jobs are executed.
       The components are: * Semantic Analyser
                           * UDF's and UDAF's
                           * Optimizer
                           * Operator
                           * Parser
                           * Execution Engine
                           * Type Checking
                           * Logical Plan Generation
                           * Physical Plan Generation
                           
6) What are the three different modes in which we can operate Hive?
Ans.--> Local mode: In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
        Distributed mode: In this mode, Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on different machines in this mode.
        Pseudo-distributed mode: This is the mode used by developers to test the code before deploying to protection. In this mode, all daemons run on same virtual machine. With this mode, we can quickly write scripts and test on limited data sets.
        
7) Features and Limitations of Hive.
Ans.--> Features.
          * Hive supports MapReduce, Spark computing Engine
          * It is a stable batch-processing framework built on top of the HDFS and can work as a data warehoues.
          * Hive Query Language uses to query structure data which is easy to code.
          * It is like SQL means it is non-procdural.
          * The structure is similar to RDBMS.
          * It also supports partitioning and bucketing.
          * It supports users to access files from HDFS
          * Capable to process very large datasets of Petabytes in size
          * It can use MapReduce code with Hive to process unstructured data.
          * JDBC/ODBC drivers are also available
          * We store Hive data on HDFS so fault tolerance is provided by Hadoop
       
       Limitations:
          * Does not support OLAP
          * It does not support update and deletion of tables
          * Subqueries are not supported
          * The latency in the apache hive query is very high
          * It is not used for real-tine data querying since it takes a while to produce a result
          * Does not support the transaction processing feature.
          
8) How to create a Database in Hive?
Ans.--> hive> create database name;

9) How to create a table in Hive?
Ans.--> hive> create table name(______) row format delimited fields terminated by ',';

10) What do you mean by describe and describe extended and describe formatted with respect to database and table.
Ans.--> Database:
                * Describe: FAILED: SemanticException [Error 10001]: Table not found hive_class_b1
                * Describe extended: FAILED: SemanticException [Error 10001]: Table not found hive_class_b1
                * Describe formatted: FAILED: SemanticException [Error 10001]: Table not found hive_class_b1
       Tables:
                * Describe: It will show names of columns and which datatype is stored in that row
                * Describe extended: It will show more detailed review than describe only, furthermore it also shows about creator of that table and time when it created and field schema of each row created in that it shows about name, type: datatype, comment and total size table type and total rows.
                * Describe formatted: It will shows detailed table information like database name, owner, createtime, lastaccesstime, protect mode, retention, location, table type, table parameters,
                  further more storage information SerDe library, Input format, Output format, Compressed, Num buckets, Bucket columns, Storage Desc Params like serialization format.
                  
 11) How to skip header rows from table in Hive?
 Ans.--> hive> set hive.cli.print.header = true;
 
 12) What is a hive operator? What are the different types of hive operators?
 Ans.--> Hive provides various built-in operators for data operations to be implimented on the tables present inside hive warehouse.
         Types of Hive Operators are:
                                    *Relational Operators
                                    *Arithmetic Operators
                                    *Logical Operators
                                    *String Operators
                                    *Operators on Complex Types
                                    
 13) Explain about the Hive Built-in Functions
 Ans.--> Functions are built to perform different analytical requirements and operations like mathematical, logical, arithmetic and relational on huge datasets and tables. Functions are used when we have to re-use similar operations multiple times.
 
 14) Write hive DDL and DML commands.
 Ans.--> DDL - Declarative method: Commands are the statements used for defining and changing the structure of a table or database.
                                 * CREATE
                                 * ALTER
                                 * DROP
                                 * TRUNCATE
        DML - Data Manipulation Language: commands are used to insert, update, retrieve, and delete data from the hive table once the table and database schema has been defined using Hive DDL commands
                                 * DELETE
                                 * MERGE
                                 * INSERT
                                 * UPDATE
                               
  15) Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.
  Ans.--> * SORT BY: To sort the rows based on the given columns per reducer
          * ORDER BY: Allows sorting of data in either ascending or descending order
          * DISTRIBUTE BY: Used on tables present in Hive. Hive uses the columns in Distribute by to distribute the rows among reducers.
          * CLUSTER BY: Cluster by used as an alternative for both Distribute By and Sort By clauses in Hive-QL. Used to Distribute the rows among reducers.
          
 16) Difference between "Internal table" and "External table" and Mention when to choose "Internal table" and "External table" in Hive?
 Ans.--> Internal Table: * It is a default table in Hive, when the user creates a table in Hive without any specifying it as external, then by default, an internal table gets created in a specific location in HDFS.
                         * Internal table will be created in a folder path similar to /user/hive/warehouse directory of Hadoop DFS.
                         * It will drop managed table or partition, the table data and the metadata associated with that table will be deleted from the HDFS.
                         
         External Table: * Hive does not manage the data of the External table and if we want to create external table then we have to mention external keyword in command.
                         * If external table created we want to use the data outside the Hive
                         * External tables are stored outside of the hive/warehouse.
                         * If we delete the external table, then only the metadata associated with the table will get deleted, the table data remains untouched by Hive

17) Where does the data of a Hive table get stored?
Ans.--> It will be stored in hive/warehouse.

18) Is it possible to change the default location of a managed table?
Ans.-->  Yes. We can change the default location of a managed table using LOCATION keyword.

19) What is metastore in Hive? What is the default database provided by Apache Hive for metastore?
Ans.--> Metastore is a system in which we store all the information about tables and relation, and all Hive implimentations need a service that will be provieded by metastore where it stores metadata.
        Derby database is the default metastore for Hive which supports only one user.
        
20) Why does Hive not store metadata information in HDFS?
Ans.--> Because, HDFS read/write operations are time consuming. So, Hive stores metadata information in the metastore using RDBMS instead of HDFS.

21) What is partition in Hive? And Why do we perform partitioning in Hive?
Ans.--> To analyse a particular set of data from any table created, not required to load entire data and desired data partition is a good approach. Hive allows partitioning the data based on particular column.

22) Difference between dynamic partitioning and static partitioning?
Ans.--> Dynamic partitioning can process entire table based on a particular column whereas Static partitioning can inserting data into individual rows. At least one static partition is must to create any partition. If partitioning a large datasets, doing a sort of extract transfrom load flow dynamic partitioning is easy and effective compared to static partitioning.

23) How do you check if a particular partition exists?
Ans.--> hive> show partitions table_name partition(partitioned_column = 'partition_value')

24) How can you stop a patition form being queried? 
Ans.--> enable offline clause with alter table statement

25) Why do we need buckets? How Hive distributes the rows into buckets?
Ans.--> To process many chunks of files, to analyse vast amount of data, sometimes burst the process and time. Bucketing is a sampling concept to analyze the data, by using hashing algorithm.
        Hive distributes the row into bucket number for row by using the formula: hash_function(bucketing_column) modulo(num_of_buckets). Here, hash_function depends on the column data type.
        
26) In Hive, how can you enable buckets?
Ans.-->  set hive.enforce.bucketing = true; can enable the process.

27) How does bucketing help in the faster execution of queries?
Ans.--> It allows faster execution of map side joins, as the data is stored in equal-sized buckets/parts and efficient sampling happens for bucketed tables when compared to non-bucketed ones.

28) How to optimize Hive performance? Explain in very detail.
Ans.--> Compression techniques reduced the amount of data being transferred and so reduces the data transfer between mappers and reducers. Furthermore ORC is great when it comes to hive performance tuning and we can improve the query performance using ORC file format easily. Using of partition is a useful concept in Hive if you have more number of columns on which you want the partitions, bucketing in the hive can be a better option.
        Hive converts the queries into different stages during execution, these stages are usually getting executed one after the other and thus increases the time of execution and Vectorization improves the query performance of all the operation like scans, aggregation, filters and joins, by performance them in batches of 1024 rows at once instead of single row each time.        

29) What is the use of Hcatalog?
Ans.--> Hadoop provides a table and storage management  layer called HCatalog. With HCatalog users in Hive can use different data processing tools like- Pig, MapReduce, Hive, Streaming etc to read and write data on the grid.
        With HCatalog we can read and write files in any format for which SerDe is available.  Default options supported by HCatalog are: RCFile, CSV, JSON and Sequence File 

30) Explain about the different types of join in Hive.
Ans.--> JOIN is a clause that is used for combining specific fields from two or more tables based on the common columns. The joins in the hive are similar to the SQL joins.
        Following are the different types:
        * JOIN: It returns the matching rows from both the tables.
        * LEFT OUTER JOIN: This type of join returns all rows from the left table even if there is no matching row in the right side. Table returns all rows from the left table and matching rows from right table. Unmatched right tables records will be NULL.
        * RIGHT OUTER JOIN: It returns all rows from the right table even if there is no matching row in the left table. Table returns all rows from the right table and matching rows from left table. Unmatched left table records will be NULL.
        * FULL OUTER JOIN: It returns all rows from the both tables that fulfill the JOIN condition. The unmatched rows from both tables will be returned as a NULL.
        
31) Is it possible to create a Cartesian join between 2 tables, using Hive?
Ans.--> NO, as this kind of JOIN cannot be implemented in MapReduce.

32) Explain the SMB Join in Hive?
Ans.--> Sort Merge Bucket(SMB) joins in the hive is for the most utilized as there are no restrictions on file or segment or table join. It can best be utilized when the tables are huge. In SMB join the sections are bucketed and arranged to utilize the join segments.

33) What is the difference between order by and sort by which one we should use?
Ans.--> Hive sort by and order by commands are used to fetch data in sorted order. The main difference b/w sort by and order by commands are the former guarantees total order in the output while the latter only guarantees ordering of rows within a reducer.
        ORDER BY: Sort the data in one reducer
        SORT BY: Sort the data within each reducer. You can use n number of reducers for sort.
        Sort by is much faster than order by.
        Because in fast case means order by maps sends each value to the single reducer and count them all.
        When it comes to sort by maps splits up the values to many reducers and each reduce generates its list and finds the count. So it can quickly operates.
        
34) What is the usefulness of the DISTRIBUTED BY clause in Hive?
Ans.--> It controls how the map output is reduced among the reducers. It is useful in case of streaming data.

35) How does data transfer happen from HDFS to Hive?
Ans.-->  Basically, the user need not LOAD DATA that moves the files to the /user/hive/warehouse/. But only if data is already present in HDFS. Hence, using the keyword external that creates the table definition in the hive metastore the user just has to define the table.
         Create external table table_name (
         id int,
         myfields string
         )
         location ‘/my/location/in/hdfs’;
 
 36) Whenever (Different Directory) I run the Hive query, it creates a new metastore_db, please explain the reason for it?
 Ans.--> Because, it creates the local metastore, while we run the hive in embedded mode and also, it looks wheather metastore already exists or not before creating the metastore. Hence, in configration file hive-site.xml. Property is "javax.jdo.option.ConnectionURL" with default value
         "jdbc:derby:;databaseName=metastore_db;create=true" this property is defined. Hence, to change the behaviour change the location to the absolute path, thus metastore will be used from that location.
         
 37) What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?
 Ans.--> The default setting for bucketing in Hive is disabled so we have to enable it by setting its value to true.
 
 38) Can a table be renamed in Hive?
 Ans.--> Yes, using alter command we can rename the table name.
         hive> alter table old_name rename to new_name;

39) Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)
Ans.--> hive> alter table table_name change column new_col int before x_col;

40) What is serde operation in HIVE?
Ans.--> Hive uses the SerDe(Serializer/Deserializer) interface for input/output. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing. A SerDe allows Hive to read in data from a table, annd write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats.
        An important concept behind Hive is that it does not own the Hadoop file system formats that data is stored in. Users are able to write files to HDFS with whatever tools/mechanism takes their fancy and use Hive to correctly "parse" that file format in a way that can be used by Hive. A SerDe is a powerful mechanism that Hive uses to "parse" data stored in HDFS to be used by Hive.
        
41) Explain how Hive Deserializes and Serialises the data?
Ans.--> A select statement creates Deserialized data i.e, columns that is understood by Hive and an insert statement creates Serialized data i.e, files that can be stored into an external storage like HDFS.

42) Write the name of the built-in SerDe in hive.
Ans.--> LazySimpleSerDe

43) What is the need of custom Serde?
Ans.--> In most cases, users want to write a Deserializer instead of a SerDe, because users just want to read their own data format instead of writing to it.
        With SerDe, Hive can read data from a table and write it to HDFS in a custom format. We can also implement custom SerDe to handle our own data formats.
        For example, the RegexDeserializer will derserialier the data using the configuration parameter 'regex', and possibly a list of column names.
        
44) Can you write the name of a complex data type(collection data types) in Hive?
Ans.--> Complex data types are built on top of Primitive Data Type. Array in Hive is an ordered sequence of similar type of elements that are indexable using the zero-based integers. Arrays in Hive are similar to the arrays in JAVA.

45) Can hive queries be executed from script files? How?
Ans.--> Shell scripts can be used to run the Hive queries in Batch mode. It will handle the input values/arguments, execution of queries and errors during the execution.
        Then the file name is given as argument to the shell script. Also we are using hive -f option and hivevar option for executing the hive queries using shell script. Lets consider that we have a below Hive query to run in batch mode. This is saved in a file with the name of input_hive_query.q.
        hive> source/path/to/file/file_with_query.hql
        
46) What are the default record and field delimiter used for hive text files?
Ans.--> The default record delimiter is -\n and the fields are -\001, \002, \003

47) How do you list all databases in Hive whose name starts with s?
Ans.--> show databases like 'p.*'

48) What is the difference between LIKE and RLIKE operators in Hive?
Ans.--> LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text.
        Ex: user_name LIKE '%Pavan'
        RLIKE is a special function in Hive where if any substring of A matches with B it evaluates to true. It also obeys Java regular experssion pattern. Users don't need to put % symbol for a simple match in RLIKE.
        Hive provides RLIKE operator that can be used for searching advanced Regular Expressions in Java.
        Ex: user_name RLIKE '.(Pavan|Jeevan).' This will return user_name that has Pavan or Jeevan in it.
        
49) How to change the column datatype in Hive?
Ans.--> hive> alter table sales_order_data change ORDERNUMBER ORDERNUMBER string;

50) How will you convert the string ’51.2’ to a float value in the particular column?
Ans.--> hive> select cast(price as float)

51) What will be the result when you cast ‘abc’ (string) as INT?
Ans.--> Conversion Failed when converting the string value 'abc' to data type int.

52) What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;
Ans.--> It creates partition on table employees with partition values coming from the columns in the select clause. It is called Dynamic partition insert.

53) Write a query where you can overwrite data in a new table from the existing table.
Ans.--> hive> insert overwrite table 
              covid19_data 
              values (12, 280, 'deaths');
              
54) What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.
Ans.--> 2 GB is the maximum size of a string data type.
        When Hive converts queries to MapReduce jobs, it decides on the appropriate key-value pairs to be used for a given record. Sequence files are in the binary format which can be split and the main use of these files is to club two or more smaller files and make them as a sequence file.
       
55) What File Formats and Applications Does Hive Support?
Ans.--> * Text file
        * Sequence file
        * ORC file
        * Row Columnar file
        * Parquet file.
        
56) How do ORC format tables help Hive to enhance its performance?
Ans.--> Hive can store the data in highly efficient manner in the Optimized RC file format. It can ease many Hive file format limitations. Using ORC files can improves the performance when reading, writing, and processing the data.

57) How can Hive avoid mapreduce while processing the query?
Ans.--> Yes, by setting command.
        hive> set hive.exec.mode.local.autoproperty = true;
        
58) What is view and indexing in hive?
Ans.--> We can create a view at the time of executing a select statement.        
        An Index is nothing but a pointer on a particular column of a table.
        
 59) Can the name of a view be the same as the name of a hive table?
 Ans.--> No, we cannot use same name for a table and view in Hive. So we have to select a unique name for a view in Hive.
 
 60) What types of costs are associated in creating indexes on hive tables?
 Ans.--> 
 
 61) Give the command to see the indexes on a table.
 Ans.--> hive> sys.indexes
 
 62) Explain the process to access subdirectories recursively in Hive queries.
 Ans.--> hive> set mapred.input.dir.recursive=true;
               set hvie.mapred.supports.subdirectories=true;
 
 63) If you run a select * query in Hive, why doesn't it run MapReduce?
 Ans.--> In Hive if yot do simple query like select * from table there will be no MapReduce job is going to run as we are just dumping the data.
 
 64) What are the uses of Hive Explode?
 Ans.--> Explode is a user defined table generating function in Hive. It takes an array as an input and outputs the elements of the array as seperate rows.
 
 65)  What is the available mechanism for connecting applications when we run Hive as a server?
 Ans.--> ODBC Driver
 
 66) Can the default location of a managed table be changed in Hive?
 Ans.--> Yes, we can change it by using -location'hdfs_path' .
 
 67) What is the Hive ObjectInspector function?
 Ans.--> Hive uses ObjectInspector to analyze the internal structure of the row object and also the structure of the individual columns.
         It provides a uniform way to access complex objects that can be stored in multiple formats in the memory, including:
         . Instance of a Java Class
         . A standard Java Object
         . A lazily-initialized object.
       
 68) What is UDF in Hive?
 Ans.--> UDF(User Defined Functions) allows the user to extend Hive Query Language. Once the UDF added in the Hive script, it works like a normal built-in function. To check which all UDFs are loaded in current hive session, we use show command.
 
 69) Write a query to extract data from hdfs to hive.
 Ans.--> hive> load data inpath '/tmp/hive_class/' into table depart_data;
 
 70) What is TextInputFormat and SequenceFileInputFormat in hive.
 Ans.--> 
 
 71) How can you prevent a large job from running for a long time in a hive?
 Ans.--> 
 
 72) When do we use explode in Hive?
 Ans.--> It is mostly used for Analysing text data using Hive. 
 
 73) Can Hive process any type of data formats? Why? Explain in very detail.
 Ans.--> Yes, Hive uses the SerDe interface for IO operations. Different SerDe interfaces can read and write any type of data. If normal directly process the data where as different type of data is in the Hadoop, Hive use different SerDe interface to process such data.
 
 74) Whenever we run a Hive query, a new metastore_db is created. Why?
 Ans.--> A local metastore is created when we run Hive in an embedded mode. Before creating, it checks whether the metastore exists or not, and this metastore property is defined inthe configuration file, hive-site.xml. The property is: javax:jdo.option.ConnectionURL wiith the default value: jdbc:derby:;databaseName=metastore_db;create=true. Therefore, we have to change the behavior of the location to an absolute path so that from that location the metastore can be used.
 
 75) Can we change the data type of a column in a hive table? Write a complete query.\
 Ans.--> Yes, hive> alter table table_name replace columns;
 
 76) While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?
 Ans.--> By eliminating Local keyword from load data statement.
 
 77) What is the precedence order in Hive configuration?
 Ans.--> * Hive SET
         * -hiveconf
         * hive-site.xml file
         * hive-default.xml file
         * hadoop-site.xml file
         * hadoop-default.xml file
         
  78) Which interface is used for accessing the Hive metastore?
  Ans.--> Thrift server is used.
  
  79) Is it possible to compress json in the Hive external table ?
  Ans.--> 
  
  80) What is the difference between local and remote metastores?
  Ans.--> *Local Mode: Basically, as the main HiveServer process, the Hive metastore service runs in the same process, but make sure Hive metastore database runs in a separate process, as well as it can be on a separate host, in Local mode.
          * Remote Mode: Whereas, the Hive metastore service runs in its own JVM process. Some processes(HiveServer2, HCatalog, Cloudera Impala™,) communicate with it with the help of the Thrift network API, in Remote mode.
                         In comparison with the Local mode, there is one benefit of using the Remote mode, that is Remote mode does not need the administrator to share JDBC login information for the metastore database along with each Hive user, but local mode does.
                         
  81) What is the purpose of archiving tables in Hive?
  Ans.--> 
  
  82) What is DBPROPERTY in Hive?
  Ans.--> DBPROPERTIES are like mentioning the details about the database created by the user.
  
  83) Differentiate between local mode and MapReduce mode in Hive.
  Ans.--> Local mode: It is used for only loading data into tables and to see tables.
          MapReduce: It is used to write queries to see data regarding to some immportant queries from the table and we can make customize the MapReduce queries where we want only one Map or Reducer we can handle that and it is time consuming.
          
    







 
